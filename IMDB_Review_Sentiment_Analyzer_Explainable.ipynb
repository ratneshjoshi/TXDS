{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\rxjos\\anaconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\rxjos\\anaconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\rxjos\\anaconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\rxjos\\anaconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\rxjos\\anaconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\rxjos\\anaconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from textblob import TextBlob\n",
    "\n",
    "import numpy as np, string, pickle, warnings, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topWords = 50000\n",
    "maxWords = 200\n",
    "nb_classes = 2\n",
    "imdbDataPicklePath = './data/imdbData.pickle'\n",
    "downloadFlag = 0\n",
    "\n",
    "if downloadFlag == 1:\n",
    "\n",
    "    # Downloading data\n",
    "    imdbData = imdb.load_data(path='imdb.npz', num_words=topWords)\n",
    "\n",
    "    # Pickle Data\n",
    "    with open(imdbDataPicklePath, 'wb') as handle:\n",
    "        pickle.dump(imdbData, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(imdbDataPicklePath, 'rb') as pHandle:\n",
    "    imdbData = pickle.load(pHandle)\n",
    "    \n",
    "(x_train, y_train), (x_test, y_test) = imdbData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " ...\n",
      " list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459])\n",
      " list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23])\n",
      " list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n"
     ]
    }
   ],
   "source": [
    "stopWords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \\\n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \\\n",
    "             'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \\\n",
    "             'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', \\\n",
    "             'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "             'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', \\\n",
    "             'above', 'below', 'to', 'from', 'off', 'over', 'then', 'here', 'there', 'when', 'where', 'why', \\\n",
    "             'how', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'own', 'same', 'so', \\\n",
    "             'than', 'too', 's', 't', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "             've', 'y', 'ma']\n",
    "word2Index = imdb.get_word_index()\n",
    "index2Word = {v: k for k, v in word2Index.items()}\n",
    "index2Word[0] = \"\"\n",
    "sentimentDict = {0: 'Negative', 1: 'Positive'}\n",
    "\n",
    "def getWordsFromIndexList(indexList):\n",
    "    wordList = []\n",
    "    for index in indexList:\n",
    "        wordList.append(index2Word[index])\n",
    "\n",
    "    return \" \".join(wordList)\n",
    "\n",
    "def getSentiment(predictArray):\n",
    "    pred = int(predictArray[0])\n",
    "    return sentimentDict[pred]\n",
    "\n",
    "def getIndexFromWordList(wordList):\n",
    "    indexList = []\n",
    "    for word in wordList:\n",
    "        print(word)\n",
    "        indexList.append(str(word2Index[word]))\n",
    "        \n",
    "    return indexList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n"
     ]
    }
   ],
   "source": [
    "print (len(word2Index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopIndexList = []\n",
    "\n",
    "for stopWord in stopWords:\n",
    "    stopIndexList.append(word2Index[stopWord])\n",
    "\n",
    "trainData = []\n",
    "\n",
    "for indexList in x_train:\n",
    "    processedList = [index for index in indexList]# if index not in stopIndexList]\n",
    "    trainData.append(processedList)\n",
    "    \n",
    "x_train = trainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Padding data to keep vectors of same size\n",
    "If size < 100 then it will be padded, else it will be cropped\n",
    "'''\n",
    "trainX = pad_sequences(x_train, maxlen = maxWords, value = 0.)\n",
    "testX = pad_sequences(x_test, maxlen = maxWords, value = 0.)\n",
    "\n",
    "'''\n",
    "One-hot encoding for the classes\n",
    "'''\n",
    "trainY = np_utils.to_categorical(y_train, num_classes = nb_classes)\n",
    "testY = np_utils.to_categorical(y_test, num_classes = nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_W, p_U, weight_decay = 0, 0, 0\n",
    "regularizer = l2(weight_decay) if weight_decay else None\n",
    "sgdOptimizer = 'adam'\n",
    "lossFun='categorical_crossentropy'\n",
    "batchSize=25\n",
    "numEpochs = 5\n",
    "numHiddenNodes = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, None, 256)         12800000  \n",
      "_________________________________________________________________\n",
      "bidi_lstm_layer (Bidirection (None, None, 512)         1050624   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 14,704,386.0\n",
      "Trainable params: 14,704,386.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(topWords, numHiddenNodes, name='embedding_layer'))\n",
    "\n",
    "model.add(Bidirectional(LSTM(numHiddenNodes, return_sequences=True,\n",
    "                            recurrent_regularizer=regularizer, kernel_regularizer=regularizer,\n",
    "                            bias_regularizer=regularizer, recurrent_dropout=p_W, dropout=p_U),\n",
    "                        merge_mode='concat', name='bidi_lstm_layer'))\n",
    "model.add(LSTM(numHiddenNodes, name = 'lstm_layer'))\n",
    "model.add(Dropout(0.5, name = 'dropout'))\n",
    "\n",
    "model.add(Dense(numHiddenNodes, name='dense_1'))\n",
    "model.add(Dense(nb_classes, name='dense_2'))\n",
    "model.add(Activation(\"softmax\"))\n",
    "# adam = Adam(lr=0.0001)\n",
    "#adadelta = Adadelta(lr=0.01, rho=0.95, epsilon=1e-08)\n",
    "model.compile(loss=lossFun, optimizer=sgdOptimizer, metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 2090s - loss: 0.4524 - acc: 0.7928 - val_loss: 0.3700 - val_acc: 0.8630\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 2211s - loss: 0.2058 - acc: 0.9236 - val_loss: 0.4441 - val_acc: 0.8219\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 2427s - loss: 0.1203 - acc: 0.9581 - val_loss: 0.4532 - val_acc: 0.8396\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 2624s - loss: 0.0753 - acc: 0.9754 - val_loss: 0.4517 - val_acc: 0.8451\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 2799s - loss: 0.0548 - acc: 0.9822 - val_loss: 0.5089 - val_acc: 0.8341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c0ca214e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, batch_size=batchSize, epochs=numEpochs, verbose=1, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 83.41%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 386s   \n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82     12500\n",
      "           1       0.79      0.91      0.85     12500\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     25000\n",
      "   macro avg       0.84      0.83      0.83     25000\n",
      "weighted avg       0.84      0.83      0.83     25000\n",
      " samples avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predY = model.predict_classes(testX)\n",
    "yPred = np_utils.to_categorical(predY, num_classes = nb_classes)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(testY, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/imdb_bi_lstm_big_tensorflow_model_ALL.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-79daf443cba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models/imdb_bi_lstm_big_tensorflow_model_ALL.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/imdb_bi_lstm_big_tensorflow_model_ALL.json'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Serialize model to JSON\n",
    "'''\n",
    "model_json = model.to_json()\n",
    "with open(\"models/imdb_bi_lstm_big_tensorflow_model_ALL.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "'''\n",
    "Serialize weights to HDF5\n",
    "'''\n",
    "model.save_weights(\"models/imdb_bi_lstm_big_tensorflow_model_ALL.h5\", overwrite=True)\n",
    "print(\"Saved model to disk...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load json and create model\n",
    "'''\n",
    "modelNum=1\n",
    "if modelNum is 1:\n",
    "    json_file = open('models/imdb_bi_lstm_big_tensorflow_model.json', 'r')\n",
    "elif modelNum is 2:\n",
    "    json_file = open('models/imdb_bi_lstm_big_tensorflow_model_NOT.json', 'r')\n",
    "elif modelNum is 3:\n",
    "    json_file = open('models/imdb_bi_lstm_big_tensorflow_model_ALL.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "'''\n",
    "Load weights into new model\n",
    "'''\n",
    "if modelNum is 1:\n",
    "    loaded_model.load_weights(\"models/imdb_bi_lstm_big_tensorflow_model.h5\")\n",
    "elif modelNum is 2:\n",
    "    loaded_model.load_weights(\"models/imdb_bi_lstm_big_tensorflow_model_NOT.h5\")\n",
    "elif modelNum is 3:\n",
    "    loaded_model.load_weights(\"models/imdb_bi_lstm_big_tensorflow_model_ALL.h5\")\n",
    "\n",
    "print (\"Loading model from disk...\")\n",
    "model = loaded_model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for test case...120\n",
      "[1. 0.]\n",
      "25000\n",
      "Text: the immediately very let reviewers roles peter lives muffin follow br communists to akin acted fracture how thought almost choice period was two of pact reynolds lines as is driven major was well see lead even by look network in character that i i least as they munchie you dealt to all takes medicine short is seriously br while before this madsen outfit\n",
      "\n",
      "Prediction: Negative\n",
      "\n",
      "Prediction is Correct\n"
     ]
    }
   ],
   "source": [
    "num = 120\n",
    "num_next = num + 1\n",
    "print(\"Testing for test case...\" + str(num))\n",
    "groundTruth = testY[num]\n",
    "print(groundTruth)\n",
    "print(len(testY))\n",
    "print\n",
    "\n",
    "sampleX = testX[num:num_next]\n",
    "predictionClass = model.predict_classes(sampleX, verbose=0)\n",
    "prediction = np_utils.to_categorical(predictionClass, num_classes = nb_classes)[0]\n",
    "\n",
    "print(\"Text: \" + str(getWordsFromIndexList(x_test[num-1])))\n",
    "print(\"\\nPrediction: \" + str(getSentiment(predictionClass)))\n",
    "if np.array_equal(groundTruth,prediction):\n",
    "    print(\"\\nPrediction is Correct\")\n",
    "else:\n",
    "    print(\"\\nPrediction is Incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Relevance for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsAndRelevances(sampleX, relevances, prediction):\n",
    "\n",
    "    unknownIndex = 0 # Index of padding\n",
    "    indexList = np.ndarray.tolist(sampleX)[0]\n",
    "    wordList = []\n",
    "    wordRelevanceList = []\n",
    "    polarityHighThreshold = 0.4\n",
    "    polarityLowThreshold = 0.0\n",
    "    \n",
    "    # Find word-wize relevance and normalize\n",
    "    wordRelevances = np.sum(relevances, -1)\n",
    "    wordRelList = np.ndarray.tolist(wordRelevances)[0]\n",
    "    \n",
    "    for i in range(maxWords):\n",
    "        index = indexList[i]\n",
    "        relevance = wordRelList[i]\n",
    "\n",
    "        if index is not unknownIndex:\n",
    "            word = index2Word[index]\n",
    "            blobText = TextBlob(word)\n",
    "            polarity = blobText.sentiment.polarity\n",
    "            prediction = int(prediction)\n",
    "#             if prediction is 0 and polarity < -0.4 or prediction is 1 and polarity > 0.4:\n",
    "            if abs(polarity) > polarityHighThreshold:\n",
    "                if prediction is 0 and polarity < 0 or prediction is 1 and polarity > 0:\n",
    "                    relevance = abs(polarity)\n",
    "                elif prediction is 0 and polarity > 0 or prediction is 1 and polarity < 0:\n",
    "                    relevance = -1 * abs(polarity)\n",
    "                    \n",
    "                print(word, polarity)\n",
    "                print(word, relevance)\n",
    "            elif abs(polarity) > polarityLowThreshold:\n",
    "                if relevance < 0 and polarity > 0 or relevance > 0 and polarity < 0:\n",
    "                    relevance = polarity\n",
    "                    print(\"Here...\")\n",
    "                    print(word, polarity)\n",
    "                    print(word, relevance)\n",
    "\n",
    "            wordList.append(index2Word[index])\n",
    "            wordRelevanceList.append(relevance)\n",
    "\n",
    "    \n",
    "    normalizedRelevanceList = [float(rel)/max(map(lambda x: abs(x), wordRelevanceList)) for rel in wordRelevanceList]\n",
    "    \n",
    "    return wordList, wordRelevanceList, normalizedRelevanceList\n",
    "\n",
    "def showWordRelevances(wordList, wordRelevanceList, normalizedRelevanceList):\n",
    "\n",
    "    print(\"\\nWord Relevances:\\n\")\n",
    "    print(\"\\nOriginal Relevance:\\n\")\n",
    "    for i in range(len(wordList)):\n",
    "        word = str(wordList[i])\n",
    "        # originalRelevance = \"{:8.2f}\".format(wordRelevanceList[i])\n",
    "        originalRelevance = wordRelevanceList[i]\n",
    "        print (\"\\t\\t\\t\" + str(originalRelevance) + \"\\t\" + word)\n",
    "\n",
    "    print(\"\\nNormalized Relevance:\\n\")\n",
    "    for i in range(len(wordList)):\n",
    "        word = str(wordList[i])\n",
    "        normalizedRelevance = \"{:8.2f}\".format(normalizedRelevanceList[i])\n",
    "        print (\"\\t\\t\\t\" + str(normalizedRelevance) + \"\\t\" + word)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process and get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunct(inputStr):\n",
    "    \n",
    "    # Remove punctuations.\n",
    "    # punctuations = string.punctuation\n",
    "    # inputStr = ''.join(ch for ch in inputStr if ch not in punctuations)\n",
    "    inputStr = inputStr.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    inputList = inputStr.lower().split()\n",
    "    return inputList\n",
    "\n",
    "def removeStopWords(inputList):\n",
    "    processedList = [word for word in inputList if word not in stopWords]\n",
    "    return processedList\n",
    "\n",
    "def preProcessQuery(inputStr):\n",
    "    return removeStopWords(removePunct(inputStr))\n",
    "#     return removePunct(inputStr)\n",
    "\n",
    "def getIndexArray(inputStr):\n",
    "    \n",
    "    words = preProcessQuery(inputStr)\n",
    "    # print(words)\n",
    "    wordIndexList = np.array([word2Index[word] if word in word2Index else 0 for word in words])\n",
    "    wordIndexArray = np.array([wordIndexList], np.int32)\n",
    "    wordIndexArray = pad_sequences(wordIndexArray, maxlen = maxWords, value = 0.)\n",
    "    return wordIndexArray\n",
    "\n",
    "def getPrediction(inputStr):\n",
    "    \n",
    "    wordIndexArray = getIndexArray(inputStr)\n",
    "    predictionScore = model.predict(wordIndexArray[0:1], verbose=0)\n",
    "    # print (predictionScore)\n",
    "    prediction = model.predict_classes(wordIndexArray[0:1], verbose=0)\n",
    "    return prediction, predictionScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Polarity from input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to test polarity: the immediately very let reviewers roles peter lives muffin follow br communists to akin acted fracture how thought almost choice period was two of pact reynolds lines as is driven major was well see lead even by look network in character that i i least as they munchie you dealt to all takes medicine short is seriously br while before this madsen outfit\n",
      "\n",
      "Prediction: Positive\n",
      "[[0.0884245  0.91157556]]\n"
     ]
    }
   ],
   "source": [
    "inputStr = input(\"Enter a sentence to test polarity: \")\n",
    "prediction, predictionScore = getPrediction(inputStr)\n",
    "print(\"\\nPrediction: \" + str(getSentiment(prediction)))\n",
    "print(predictionScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance and HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DeepExplain Session to get word relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Relevances Generated...\n"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    \n",
    "    '''\n",
    "    Need to reconstruct the graph in DeepExplain context, using the same weights.\n",
    "    1. Get the input tensor\n",
    "    2. Get embedding tensor\n",
    "    3. Target the output of the last dense layer (pre-softmax)\n",
    "    '''\n",
    "    \n",
    "    inputTensor = model.layers[0].input\n",
    "    embeddingTensor = model.layers[0].output\n",
    "    preSoftmax = model.layers[-2].output\n",
    "    \n",
    "    # Sample Data for attribution\n",
    "    wordIndexArray = getIndexArray(inputStr)\n",
    "    sampleX = pad_sequences(wordIndexArray, maxlen = maxWords, value = 0.)\n",
    "    \n",
    "    # Perform Embedding Lookup\n",
    "    getEmbeddingOutput = K.function([inputTensor],[embeddingTensor])\n",
    "    embeddingOutput = getEmbeddingOutput([sampleX])[0]\n",
    "    \n",
    "    # Get Prediction for attribution\n",
    "    prediction, predictionScore = getPrediction(inputStr)\n",
    "    ys = np_utils.to_categorical(prediction, num_classes = nb_classes)\n",
    "    \n",
    "    #relevances = de.explain('grad*input', targetTensor * ys, inputTensor, sampleX)\n",
    "    #relevances = de.explain('saliency', targetTensor * ys, inputTensor, sampleX)\n",
    "    #relevances = de.explain('intgrad', targetTensor * ys, inputTensor, sampleX)\n",
    "    #relevances = de.explain('deeplift', targetTensor * ys, inputTensor, sampleX)\n",
    "    relevances = de.explain('elrp', preSoftmax * ys, embeddingTensor, embeddingOutput)\n",
    "    #relevances = de.explain('occlusion', targetTensor * ys, inputTensor, sampleX)\n",
    "    print (\"Feature Relevances Generated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here...\n",
      "major 0.0625\n",
      "major 0.0625\n",
      "Here...\n",
      "least -0.3\n",
      "least -0.3\n",
      "Here...\n",
      "seriously -0.3333333333333333\n",
      "seriously -0.3333333333333333\n",
      "\n",
      "Word Relevances:\n",
      "\n",
      "\n",
      "Original Relevance:\n",
      "\n",
      "\t\t\t0.1014721617102623\timmediately\n",
      "\t\t\t0.06040450930595398\tvery\n",
      "\t\t\t-0.11613145470619202\tlet\n",
      "\t\t\t0.16224980354309082\treviewers\n",
      "\t\t\t-0.1106242835521698\troles\n",
      "\t\t\t-0.053366512060165405\tpeter\n",
      "\t\t\t0.007471807301044464\tlives\n",
      "\t\t\t0.1293599009513855\tmuffin\n",
      "\t\t\t0.057078223675489426\tfollow\n",
      "\t\t\t-0.05188463628292084\tbr\n",
      "\t\t\t-0.09221111238002777\tcommunists\n",
      "\t\t\t0.038896847516298294\takin\n",
      "\t\t\t-0.05819696560502052\tacted\n",
      "\t\t\t-0.06760017573833466\tfracture\n",
      "\t\t\t-0.007680210750550032\tthought\n",
      "\t\t\t-0.012866023927927017\talmost\n",
      "\t\t\t0.10264774411916733\tchoice\n",
      "\t\t\t0.028989844024181366\tperiod\n",
      "\t\t\t0.005998705513775349\ttwo\n",
      "\t\t\t0.04465288668870926\tpact\n",
      "\t\t\t0.06110542640089989\treynolds\n",
      "\t\t\t0.0384274423122406\tlines\n",
      "\t\t\t-0.2126723825931549\tdriven\n",
      "\t\t\t0.0625\tmajor\n",
      "\t\t\t0.0199551060795784\twell\n",
      "\t\t\t-0.0008040880784392357\tsee\n",
      "\t\t\t-0.16499650478363037\tlead\n",
      "\t\t\t-0.10735148191452026\teven\n",
      "\t\t\t0.09889013320207596\tlook\n",
      "\t\t\t-0.5283183455467224\tnetwork\n",
      "\t\t\t-0.04262220859527588\tin\n",
      "\t\t\t0.0874721109867096\tcharacter\n",
      "\t\t\t-0.3\tleast\n",
      "\t\t\t0.21113628149032593\tmunchie\n",
      "\t\t\t-0.23325678706169128\tdealt\n",
      "\t\t\t-0.0070510609075427055\tall\n",
      "\t\t\t-0.10313568264245987\ttakes\n",
      "\t\t\t0.24939045310020447\tmedicine\n",
      "\t\t\t0.016399625688791275\tshort\n",
      "\t\t\t-0.3333333333333333\tseriously\n",
      "\t\t\t-0.09522618353366852\tbr\n",
      "\t\t\t-0.34574800729751587\tmadsen\n",
      "\t\t\t-0.3586016893386841\toutfit\n",
      "\n",
      "Normalized Relevance:\n",
      "\n",
      "\t\t\t    0.19\timmediately\n",
      "\t\t\t    0.11\tvery\n",
      "\t\t\t   -0.22\tlet\n",
      "\t\t\t    0.31\treviewers\n",
      "\t\t\t   -0.21\troles\n",
      "\t\t\t   -0.10\tpeter\n",
      "\t\t\t    0.01\tlives\n",
      "\t\t\t    0.24\tmuffin\n",
      "\t\t\t    0.11\tfollow\n",
      "\t\t\t   -0.10\tbr\n",
      "\t\t\t   -0.17\tcommunists\n",
      "\t\t\t    0.07\takin\n",
      "\t\t\t   -0.11\tacted\n",
      "\t\t\t   -0.13\tfracture\n",
      "\t\t\t   -0.01\tthought\n",
      "\t\t\t   -0.02\talmost\n",
      "\t\t\t    0.19\tchoice\n",
      "\t\t\t    0.05\tperiod\n",
      "\t\t\t    0.01\ttwo\n",
      "\t\t\t    0.08\tpact\n",
      "\t\t\t    0.12\treynolds\n",
      "\t\t\t    0.07\tlines\n",
      "\t\t\t   -0.40\tdriven\n",
      "\t\t\t    0.12\tmajor\n",
      "\t\t\t    0.04\twell\n",
      "\t\t\t   -0.00\tsee\n",
      "\t\t\t   -0.31\tlead\n",
      "\t\t\t   -0.20\teven\n",
      "\t\t\t    0.19\tlook\n",
      "\t\t\t   -1.00\tnetwork\n",
      "\t\t\t   -0.08\tin\n",
      "\t\t\t    0.17\tcharacter\n",
      "\t\t\t   -0.57\tleast\n",
      "\t\t\t    0.40\tmunchie\n",
      "\t\t\t   -0.44\tdealt\n",
      "\t\t\t   -0.01\tall\n",
      "\t\t\t   -0.20\ttakes\n",
      "\t\t\t    0.47\tmedicine\n",
      "\t\t\t    0.03\tshort\n",
      "\t\t\t   -0.63\tseriously\n",
      "\t\t\t   -0.18\tbr\n",
      "\t\t\t   -0.65\tmadsen\n",
      "\t\t\t   -0.68\toutfit\n"
     ]
    }
   ],
   "source": [
    "wordList, wordRelevanceList, normalizedRelevanceList = getWordsAndRelevances(sampleX, relevances, prediction[0])\n",
    "showWordRelevances(wordList, wordRelevanceList, normalizedRelevanceList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LRP heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ceceff\">immediately</span> <span style=\"background-color:#e2e2ff\">very</span> <span style=\"background-color:#ffc6c6\">let</span> <span style=\"background-color:#b0b0ff\">reviewers</span> <span style=\"background-color:#ffcaca\">roles</span> <span style=\"background-color:#ffe6e6\">peter</span> <span style=\"background-color:#fcfcff\">lives</span> <span style=\"background-color:#c0c0ff\">muffin</span> <span style=\"background-color:#e3e3ff\">follow</span> <span style=\"background-color:#ffe6e6\">br</span> <span style=\"background-color:#ffd2d2\">communists</span> <span style=\"background-color:#ececff\">akin</span> <span style=\"background-color:#ffe2e2\">acted</span> <span style=\"background-color:#ffdede\">fracture</span> <span style=\"background-color:#fffcfc\">thought</span> <span style=\"background-color:#fff8f8\">almost</span> <span style=\"background-color:#ceceff\">choice</span> <span style=\"background-color:#f0f0ff\">period</span> <span style=\"background-color:#fcfcff\">two</span> <span style=\"background-color:#eaeaff\">pact</span> <span style=\"background-color:#e2e2ff\">reynolds</span> <span style=\"background-color:#ececff\">lines</span> <span style=\"background-color:#ff9898\">driven</span> <span style=\"background-color:#e0e0ff\">major</span> <span style=\"background-color:#f6f6ff\">well</span> <span style=\"background-color:#fffefe\">see</span> <span style=\"background-color:#ffb0b0\">lead</span> <span style=\"background-color:#ffcaca\">even</span> <span style=\"background-color:#d0d0ff\">look</span> <span style=\"background-color:#ff0000\">network</span> <span style=\"background-color:#ffeaea\">in</span> <span style=\"background-color:#d3d3ff\">character</span> <span style=\"background-color:#ff6e6e\">least</span> <span style=\"background-color:#9898ff\">munchie</span> <span style=\"background-color:#ff8e8e\">dealt</span> <span style=\"background-color:#fffcfc\">all</span> <span style=\"background-color:#ffcece\">takes</span> <span style=\"background-color:#8686ff\">medicine</span> <span style=\"background-color:#f8f8ff\">short</span> <span style=\"background-color:#ff5e5e\">seriously</span> <span style=\"background-color:#ffd0d0\">br</span> <span style=\"background-color:#ff5858\">madsen</span> <span style=\"background-color:#ff5151\">outfit</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rescale_score_by_abs (score, max_score, min_score):\n",
    "    \"\"\"\n",
    "    rescale positive score to the range [0.5, 1.0], negative score to the range [0.0, 0.5],\n",
    "    using the extremal scores max_score and min_score for normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    # CASE 1: positive AND negative scores occur --------------------\n",
    "    if max_score>0 and min_score<0:\n",
    "    \n",
    "        if max_score >= abs(min_score):   # deepest color is positive\n",
    "            if score>=0:\n",
    "                return 0.5 + 0.5*(score/max_score)\n",
    "            else:\n",
    "                return 0.5 - 0.5*(abs(score)/max_score)\n",
    "\n",
    "        else:                             # deepest color is negative\n",
    "            if score>=0:\n",
    "                return 0.5 + 0.5*(score/abs(min_score))\n",
    "            else:\n",
    "                return 0.5 - 0.5*(score/min_score)   \n",
    "    \n",
    "    # CASE 2: ONLY positive scores occur -----------------------------       \n",
    "    elif max_score>0 and min_score>=0: \n",
    "        if max_score == min_score:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.5 + 0.5*(score/max_score)\n",
    "    \n",
    "    # CASE 3: ONLY negative scores occur -----------------------------\n",
    "    elif max_score<=0 and min_score<0: \n",
    "        if max_score == min_score:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 0.5 - 0.5*(score/min_score)    \n",
    "  \n",
    "      \n",
    "def getRGB (c_tuple):\n",
    "    return \"#%02x%02x%02x\"%(int(c_tuple[0]*255), int(c_tuple[1]*255), int(c_tuple[2]*255))\n",
    "\n",
    "     \n",
    "def span_word (word, score, colormap):\n",
    "    return \"<span style=\\\"background-color:\"+getRGB(colormap(score))+\"\\\">\"+word+\"</span>\"\n",
    "\n",
    "\n",
    "def html_heatmap (words, scores, cmap_name=\"bwr\"):\n",
    "    \n",
    "    colormap  = plt.get_cmap(cmap_name)\n",
    "     \n",
    "    assert len(words)==len(scores)\n",
    "    max_s     = max(scores)\n",
    "    min_s     = min(scores)\n",
    "    \n",
    "    output_text = \"\"\n",
    "    \n",
    "    for idx, w in enumerate(words):\n",
    "        score       = rescale_score_by_abs(scores[idx], max_s, min_s)\n",
    "        output_text = output_text + span_word(w, score, colormap) + \" \"\n",
    "\n",
    "    return output_text + \"\\n\"\n",
    "            \n",
    "relevanceScores = [-1.00 * scr for scr in wordRelevanceList]\n",
    "htmlHeatMap = html_heatmap(wordList, relevanceScores)\n",
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(htmlHeatMap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "xai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
